{
  CollectionAdminRequest.createCollection("destinationCollection","ml",2,1).process(cluster.getSolrClient());
  AbstractDistribZkTestBase.waitForRecoveriesToFinish("destinationCollection",cluster.getSolrClient().getZkStateReader(),false,true,TIMEOUT);
  UpdateRequest updateRequest=new UpdateRequest();
  for (int i=0; i < 5000; i+=2) {
    updateRequest.add(id,String.valueOf(i),"tv_text","a b c c d","out_i","1");
    updateRequest.add(id,String.valueOf(i + 1),"tv_text","a b e e f","out_i","0");
  }
  updateRequest.commit(cluster.getSolrClient(),COLLECTION);
  StreamExpression expression;
  TupleStream stream;
  List<Tuple> tuples;
  StreamFactory factory=new StreamFactory().withCollectionZkHost("collection1",cluster.getZkServer().getZkAddress()).withCollectionZkHost("destinationCollection",cluster.getZkServer().getZkAddress()).withFunctionName("features",FeaturesSelectionStream.class).withFunctionName("train",TextLogitStream.class).withFunctionName("search",CloudSolrStream.class).withFunctionName("update",UpdateStream.class);
  expression=StreamExpressionParser.parse("features(collection1, q=\"*:*\", featureSet=\"first\", field=\"tv_text\", outcome=\"out_i\", numTerms=4)");
  stream=new FeaturesSelectionStream(expression,factory);
  tuples=getTuples(stream);
  assert(tuples.size() == 4);
  HashSet<String> terms=new HashSet<>();
  for (  Tuple tuple : tuples) {
    terms.add((String)tuple.get("term_s"));
  }
  assertTrue(terms.contains("d"));
  assertTrue(terms.contains("c"));
  assertTrue(terms.contains("e"));
  assertTrue(terms.contains("f"));
  String textLogitExpression="train(" + "collection1, " + "features(collection1, q=\"*:*\", featureSet=\"first\", field=\"tv_text\", outcome=\"out_i\", numTerms=4),"+ "q=\"*:*\", "+ "name=\"model\", "+ "field=\"tv_text\", "+ "outcome=\"out_i\", "+ "maxIterations=100)";
  stream=factory.constructStream(textLogitExpression);
  tuples=getTuples(stream);
  Tuple lastTuple=tuples.get(tuples.size() - 1);
  List<Double> lastWeights=lastTuple.getDoubles("weights_ds");
  Double[] lastWeightsArray=lastWeights.toArray(new Double[lastWeights.size()]);
  Double[] testRecord={1.0,1.17,0.691,0.0,0.0};
  double d=sum(multiply(testRecord,lastWeightsArray));
  double prob=sigmoid(d);
  assertEquals(prob,1.0,0.1);
  Double[] testRecord2={1.0,0.0,0.0,1.17,0.691};
  d=sum(multiply(testRecord2,lastWeightsArray));
  prob=sigmoid(d);
  assertEquals(prob,0,0.1);
  stream=factory.constructStream("update(destinationCollection, batchSize=5, " + textLogitExpression + ")");
  getTuples(stream);
  cluster.getSolrClient().commit("destinationCollection");
  stream=factory.constructStream("search(destinationCollection, " + "q=*:*, " + "fl=\"iteration_i,* \", "+ "rows=100, "+ "sort=\"iteration_i desc\")");
  tuples=getTuples(stream);
  assertEquals(100,tuples.size());
  Tuple lastModel=tuples.get(0);
  ClassificationEvaluation evaluation=ClassificationEvaluation.create(lastModel.fields);
  assertTrue(evaluation.getF1() >= 1.0);
  assertEquals(Math.log(5000.0 / (2500 + 1)),lastModel.getDoubles("idfs_ds").get(0),0.0001);
  Tuple firstTuple=tuples.get(99);
  assertEquals(1L,(long)firstTuple.getLong("iteration_i"));
  CollectionAdminRequest.deleteCollection("destinationCollection").process(cluster.getSolrClient());
}
