{
  fieldState.reset();
  final boolean doInvert=consumer.start(fields,count);
  for (int i=0; i < count; i++) {
    final IndexableField field=fields[i];
    final IndexableFieldType fieldType=field.fieldType();
    if (doInvert) {
      final boolean analyzed=fieldType.tokenized() && docState.analyzer != null;
      if (fieldType.omitNorms() && field.boost() != 1.0f) {
        throw new UnsupportedOperationException("You cannot set an index-time boost: norms are omitted for field '" + field.name() + "'");
      }
      final boolean checkOffsets=fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
      int lastStartOffset=0;
      if (i > 0) {
        fieldState.position+=analyzed ? docState.analyzer.getPositionIncrementGap(fieldInfo.name) : 0;
      }
      boolean succeededInProcessingField=false;
      try (TokenStream stream=field.tokenStream(docState.analyzer)){
        stream.reset();
        boolean hasMoreTokens=stream.incrementToken();
        fieldState.attributeSource=stream;
        OffsetAttribute offsetAttribute=fieldState.attributeSource.addAttribute(OffsetAttribute.class);
        PositionIncrementAttribute posIncrAttribute=fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);
        if (hasMoreTokens) {
          consumer.start(field);
          do {
            final int posIncr=posIncrAttribute.getPositionIncrement();
            if (posIncr < 0) {
              throw new IllegalArgumentException("position increment must be >=0 (got " + posIncr + ") for field '"+ field.name()+ "'");
            }
            if (fieldState.position == 0 && posIncr == 0) {
              throw new IllegalArgumentException("first position increment must be > 0 (got 0) for field '" + field.name() + "'");
            }
            int position=fieldState.position + posIncr;
            if (position > 0) {
              position--;
            }
 else             if (position < 0) {
              throw new IllegalArgumentException("position overflow for field '" + field.name() + "'");
            }
            fieldState.position=position;
            if (posIncr == 0)             fieldState.numOverlap++;
            if (checkOffsets) {
              int startOffset=fieldState.offset + offsetAttribute.startOffset();
              int endOffset=fieldState.offset + offsetAttribute.endOffset();
              if (startOffset < 0 || endOffset < startOffset) {
                throw new IllegalArgumentException("startOffset must be non-negative, and endOffset must be >= startOffset, " + "startOffset=" + startOffset + ",endOffset="+ endOffset+ " for field '"+ field.name()+ "'");
              }
              if (startOffset < lastStartOffset) {
                throw new IllegalArgumentException("offsets must not go backwards startOffset=" + startOffset + " is < lastStartOffset="+ lastStartOffset+ " for field '"+ field.name()+ "'");
              }
              lastStartOffset=startOffset;
            }
            boolean success=false;
            try {
              consumer.add();
              success=true;
            }
  finally {
              if (!success) {
                docState.docWriter.setAborting();
              }
            }
            fieldState.length++;
            fieldState.position++;
          }
 while (stream.incrementToken());
        }
        stream.end();
        fieldState.position+=posIncrAttribute.getPositionIncrement();
        fieldState.offset+=offsetAttribute.endOffset();
        if (docState.maxTermPrefix != null) {
          final String msg="Document contains at least one immense term in field=\"" + fieldInfo.name + "\" (whose UTF8 encoding is longer than the max length "+ DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8+ "), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '"+ docState.maxTermPrefix+ "...'";
          if (docState.infoStream.isEnabled("IW")) {
            docState.infoStream.message("IW","ERROR: " + msg);
          }
          docState.maxTermPrefix=null;
          throw new IllegalArgumentException(msg);
        }
        succeededInProcessingField=true;
      }
  finally {
        if (!succeededInProcessingField && docState.infoStream.isEnabled("DW")) {
          docState.infoStream.message("DW","An exception was thrown while processing field " + fieldInfo.name);
        }
      }
      fieldState.offset+=analyzed ? docState.analyzer.getOffsetGap(fieldInfo.name) : 0;
      fieldState.boost*=field.boost();
    }
    fields[i]=null;
  }
  consumer.finish();
  endConsumer.finish();
}
