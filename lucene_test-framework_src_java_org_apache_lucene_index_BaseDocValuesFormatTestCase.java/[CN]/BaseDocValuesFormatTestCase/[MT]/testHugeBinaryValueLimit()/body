{
  assumeFalse("test requires codec with limits on max binary field length",codecAcceptsHugeBinaryValues("field"));
  Analyzer analyzer=new MockAnalyzer(random());
  Directory d=newFSDirectory(TestUtil.getTempDir("hugeBinaryValues"));
  boolean doFixed=random().nextBoolean();
  int numDocs;
  int fixedLength=0;
  if (doFixed) {
    numDocs=TestUtil.nextInt(random(),10,20);
    fixedLength=Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;
  }
 else {
    numDocs=TestUtil.nextInt(random(),100,200);
  }
  IndexWriter w=new IndexWriter(d,newIndexWriterConfig(TEST_VERSION_CURRENT,analyzer));
  List<byte[]> docBytes=new ArrayList<byte[]>();
  long totalBytes=0;
  for (int docID=0; docID < numDocs; docID++) {
    int numBytes;
    if (doFixed) {
      numBytes=fixedLength;
    }
 else     if (docID == 0 || random().nextInt(5) == 3) {
      numBytes=Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;
    }
 else {
      numBytes=TestUtil.nextInt(random(),1,Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
    }
    totalBytes+=numBytes;
    if (totalBytes > 5 * 1024 * 1024) {
      break;
    }
    byte[] bytes=new byte[numBytes];
    random().nextBytes(bytes);
    docBytes.add(bytes);
    Document doc=new Document();
    BytesRef b=new BytesRef(bytes);
    b.length=bytes.length;
    doc.add(new BinaryDocValuesField("field",b));
    doc.add(new StringField("id","" + docID,Field.Store.YES));
    w.addDocument(doc);
  }
  DirectoryReader r=w.getReader();
  w.close();
  AtomicReader ar=SlowCompositeReaderWrapper.wrap(r);
  BinaryDocValues s=FieldCache.DEFAULT.getTerms(ar,"field",false);
  for (int docID=0; docID < docBytes.size(); docID++) {
    StoredDocument doc=ar.document(docID);
    BytesRef bytes=new BytesRef();
    s.get(docID,bytes);
    byte[] expected=docBytes.get(Integer.parseInt(doc.get("id")));
    assertEquals(expected.length,bytes.length);
    assertEquals(new BytesRef(expected),bytes);
  }
  ar.close();
  d.close();
}
