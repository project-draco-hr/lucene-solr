{
  super.setUp();
  Arrays.sort(testTerms);
  int tokenUpto=0;
  for (int i=0; i < testTerms.length; i++) {
    positions[i]=new int[TERM_FREQ];
    offsets[i]=new TermVectorOffsetInfo[TERM_FREQ];
    for (int j=0; j < TERM_FREQ; j++) {
      positions[i][j]=(int)(j * 10 + Math.random() * 10);
      offsets[i][j]=new TermVectorOffsetInfo(j * 10,j * 10 + testTerms[i].length());
      TestToken token=tokens[tokenUpto++]=new TestToken();
      token.text=testTerms[i];
      token.pos=positions[i][j];
      token.startOffset=offsets[i][j].getStartOffset();
      token.endOffset=offsets[i][j].getEndOffset();
    }
  }
  Arrays.sort(tokens);
  dir=newDirectory();
  IndexWriter writer=new IndexWriter(dir,newIndexWriterConfig(TEST_VERSION_CURRENT,new MyAnalyzer()).setMaxBufferedDocs(-1).setMergePolicy(newLogMergePolicy(false,10)));
  Document doc=new Document();
  for (int i=0; i < testFields.length; i++) {
    final Field.TermVector tv;
    if (testFieldsStorePos[i] && testFieldsStoreOff[i])     tv=Field.TermVector.WITH_POSITIONS_OFFSETS;
 else     if (testFieldsStorePos[i] && !testFieldsStoreOff[i])     tv=Field.TermVector.WITH_POSITIONS;
 else     if (!testFieldsStorePos[i] && testFieldsStoreOff[i])     tv=Field.TermVector.WITH_OFFSETS;
 else     tv=Field.TermVector.YES;
    doc.add(new Field(testFields[i],"",Field.Store.NO,Field.Index.ANALYZED,tv));
  }
  for (int j=0; j < 5; j++)   writer.addDocument(doc);
  writer.commit();
  seg=writer.newestSegment().name;
  writer.close();
  fieldInfos=new FieldInfos(dir,IndexFileNames.segmentFileName(seg,"",IndexFileNames.FIELD_INFOS_EXTENSION));
}
