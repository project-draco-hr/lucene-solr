{
  consumer.startDocument();
  storedConsumer.startDocument();
  fieldCount=0;
  final int thisFieldGen=fieldGen++;
  for (  IndexableField field : docState.doc.indexableFields()) {
    final String fieldName=field.name();
    IndexableFieldType ft=field.fieldType();
    DocFieldProcessorPerField fp=processField(fieldInfos,thisFieldGen,fieldName,ft);
    fp.addField(field);
  }
  for (  StorableField field : docState.doc.storableFields()) {
    final String fieldName=field.name();
    IndexableFieldType ft=field.fieldType();
    FieldInfo fieldInfo=fieldInfos.addOrUpdate(fieldName,ft);
    storedConsumer.addField(docState.docID,field,fieldInfo);
  }
  ArrayUtil.introSort(fields,0,fieldCount,fieldsComp);
  for (int i=0; i < fieldCount; i++) {
    final DocFieldProcessorPerField perField=fields[i];
    perField.consumer.processFields(perField.fields,perField.fieldCount);
  }
  if (docState.maxTermPrefix != null && docState.infoStream.isEnabled("IW")) {
    docState.infoStream.message("IW","WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length " + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + "), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '"+ docState.maxTermPrefix+ "...'");
    docState.maxTermPrefix=null;
  }
}
