{
  RAMDirectory d=new MockRAMDirectory();
  IndexWriter writer=new IndexWriter(d,new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),true,IndexWriter.MaxFieldLength.LIMITED);
  for (int i=0; i < 5 * writer.getMergeFactor(); i++) {
    Document doc=new Document();
    doc.add(new Field("tvnot","one two two three three three",Field.Store.YES,Field.Index.ANALYZED,Field.TermVector.NO));
    doc.add(new Field("termvector","one two two three three three",Field.Store.YES,Field.Index.ANALYZED,Field.TermVector.YES));
    doc.add(new Field("tvoffset","one two two three three three",Field.Store.YES,Field.Index.ANALYZED,Field.TermVector.WITH_OFFSETS));
    doc.add(new Field("tvposition","one two two three three three",Field.Store.YES,Field.Index.ANALYZED,Field.TermVector.WITH_POSITIONS));
    doc.add(new Field("tvpositionoffset","one two two three three three",Field.Store.YES,Field.Index.ANALYZED,Field.TermVector.WITH_POSITIONS_OFFSETS));
    writer.addDocument(doc);
  }
  writer.close();
  IndexReader reader=IndexReader.open(d,false);
  FieldSortedTermVectorMapper mapper=new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
  reader.getTermFreqVector(0,mapper);
  Map<String,SortedSet<TermVectorEntry>> map=mapper.getFieldToTerms();
  assertTrue("map is null and it shouldn't be",map != null);
  assertTrue("map Size: " + map.size() + " is not: "+ 4,map.size() == 4);
  Set<TermVectorEntry> set=map.get("termvector");
  for (Iterator<TermVectorEntry> iterator=set.iterator(); iterator.hasNext(); ) {
    TermVectorEntry entry=iterator.next();
    assertTrue("entry is null and it shouldn't be",entry != null);
    System.out.println("Entry: " + entry);
  }
}
