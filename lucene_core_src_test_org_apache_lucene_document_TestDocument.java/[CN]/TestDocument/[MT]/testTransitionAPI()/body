{
  Directory dir=newDirectory();
  RandomIndexWriter w=new RandomIndexWriter(random,dir);
  Document doc=new Document();
  doc.add(new Field("stored","abc",Field.Store.YES,Field.Index.NO));
  doc.add(new Field("stored_indexed","abc xyz",Field.Store.YES,Field.Index.NOT_ANALYZED));
  doc.add(new Field("stored_tokenized","abc xyz",Field.Store.YES,Field.Index.ANALYZED));
  doc.add(new Field("indexed","abc xyz",Field.Store.NO,Field.Index.NOT_ANALYZED));
  doc.add(new Field("tokenized","abc xyz",Field.Store.NO,Field.Index.ANALYZED));
  doc.add(new Field("tokenized_reader",new StringReader("abc xyz")));
  doc.add(new Field("tokenized_tokenstream",w.w.getAnalyzer().tokenStream("tokenized_tokenstream",new StringReader("abc xyz"))));
  doc.add(new Field("binary",new byte[10]));
  doc.add(new Field("tv","abc xyz",Field.Store.NO,Field.Index.ANALYZED,Field.TermVector.YES));
  doc.add(new Field("tv_pos","abc xyz",Field.Store.NO,Field.Index.ANALYZED,Field.TermVector.WITH_POSITIONS));
  doc.add(new Field("tv_off","abc xyz",Field.Store.NO,Field.Index.ANALYZED,Field.TermVector.WITH_OFFSETS));
  doc.add(new Field("tv_pos_off","abc xyz",Field.Store.NO,Field.Index.ANALYZED,Field.TermVector.WITH_POSITIONS_OFFSETS));
  w.addDocument(doc);
  IndexReader r=w.getReader();
  w.close();
  doc=r.document(0);
  assertEquals(4,doc.getFields().size());
  assertEquals("abc",doc.get("stored"));
  assertEquals("abc xyz",doc.get("stored_indexed"));
  assertEquals("abc xyz",doc.get("stored_tokenized"));
  final BytesRef br=doc.getBinaryValue("binary");
  assertNotNull(br);
  assertEquals(10,br.length);
  IndexSearcher s=new IndexSearcher(r);
  assertEquals(1,s.search(new TermQuery(new Term("stored_indexed","abc xyz")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("stored_tokenized","abc")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("stored_tokenized","xyz")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("indexed","abc xyz")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("tokenized","abc")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("tokenized","xyz")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("tokenized_reader","abc")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("tokenized_reader","xyz")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("tokenized_tokenstream","abc")),1).totalHits);
  assertEquals(1,s.search(new TermQuery(new Term("tokenized_tokenstream","xyz")),1).totalHits);
  for (  String field : new String[]{"tv","tv_pos","tv_off","tv_pos_off"}) {
    InvertedFields tvFields=r.getTermVectors(0);
    Terms tvs=tvFields.terms(field);
    assertNotNull(tvs);
    assertEquals(2,tvs.getUniqueTermCount());
    TermsEnum tvsEnum=tvs.iterator(null);
    assertEquals(new BytesRef("abc"),tvsEnum.next());
    final DocsAndPositionsEnum dpEnum=tvsEnum.docsAndPositions(null,null,false);
    if (field.equals("tv")) {
      assertNull(dpEnum);
    }
 else {
      assertNotNull(dpEnum);
    }
    assertEquals(new BytesRef("xyz"),tvsEnum.next());
    assertNull(tvsEnum.next());
  }
  r.close();
  dir.close();
}
