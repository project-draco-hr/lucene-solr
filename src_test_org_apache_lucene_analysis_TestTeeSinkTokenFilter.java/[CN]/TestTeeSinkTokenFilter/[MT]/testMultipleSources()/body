{
  final TeeSinkTokenFilter tee1=new TeeSinkTokenFilter(new WhitespaceTokenizer(new StringReader(buffer1.toString())));
  final TeeSinkTokenFilter.SinkTokenStream dogDetector=tee1.newSinkTokenStream(dogFilter);
  final TeeSinkTokenFilter.SinkTokenStream theDetector=tee1.newSinkTokenStream(theFilter);
  final TokenStream source1=new CachingTokenFilter(tee1);
  tee1.addAttribute(CheckClearAttributesAttribute.class);
  dogDetector.addAttribute(CheckClearAttributesAttribute.class);
  theDetector.addAttribute(CheckClearAttributesAttribute.class);
  final TeeSinkTokenFilter tee2=new TeeSinkTokenFilter(new WhitespaceTokenizer(new StringReader(buffer2.toString())));
  tee2.addSinkTokenStream(dogDetector);
  tee2.addSinkTokenStream(theDetector);
  final TokenStream source2=tee2;
  assertTokenStreamContents(source1,tokens1);
  assertTokenStreamContents(source2,tokens2);
  assertTokenStreamContents(theDetector,new String[]{"The","the","The","the"});
  assertTokenStreamContents(dogDetector,new String[]{"Dogs","Dogs"});
  source1.reset();
  TokenStream lowerCasing=new LowerCaseFilter(Version.LUCENE_CURRENT,source1);
  String[] lowerCaseTokens=new String[tokens1.length];
  for (int i=0; i < tokens1.length; i++)   lowerCaseTokens[i]=tokens1[i].toLowerCase();
  assertTokenStreamContents(lowerCasing,lowerCaseTokens);
}
