{
  final int flags=GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL| SPLIT_ON_CASE_CHANGE| SPLIT_ON_NUMERICS| STEM_ENGLISH_POSSESSIVE;
  final CharArraySet protWords=new CharArraySet(new HashSet<>(Arrays.asList("NUTCH")),false);
  Analyzer a=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field){
      Tokenizer tokenizer=new MockTokenizer(MockTokenizer.WHITESPACE,false);
      return new TokenStreamComponents(tokenizer,new WordDelimiterFilter(tokenizer,flags,protWords));
    }
  }
;
  assertAnalyzesTo(a,"LUCENE / SOLR",new String[]{"LUCENE","SOLR"},new int[]{0,9},new int[]{6,13},new int[]{1,1});
  assertAnalyzesTo(a,"LUCENE / solR",new String[]{"LUCENE","sol","solR","R"},new int[]{0,9,9,12},new int[]{6,12,13,13},new int[]{1,1,0,1});
  assertAnalyzesTo(a,"LUCENE / NUTCH SOLR",new String[]{"LUCENE","NUTCH","SOLR"},new int[]{0,9,15},new int[]{6,14,19},new int[]{1,1,1});
  Analyzer a2=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field){
      Tokenizer tokenizer=new MockTokenizer(MockTokenizer.WHITESPACE,false);
      return new TokenStreamComponents(tokenizer,new WordDelimiterFilter(new LargePosIncTokenFilter(tokenizer),flags,protWords));
    }
  }
;
  assertAnalyzesTo(a2,"LUCENE largegap SOLR",new String[]{"LUCENE","largegap","SOLR"},new int[]{0,7,16},new int[]{6,15,20},new int[]{1,10,1});
  assertAnalyzesTo(a2,"LUCENE / SOLR",new String[]{"LUCENE","SOLR"},new int[]{0,9},new int[]{6,13},new int[]{1,11});
  assertAnalyzesTo(a2,"LUCENE / solR",new String[]{"LUCENE","sol","solR","R"},new int[]{0,9,9,12},new int[]{6,12,13,13},new int[]{1,11,0,1});
  assertAnalyzesTo(a2,"LUCENE / NUTCH SOLR",new String[]{"LUCENE","NUTCH","SOLR"},new int[]{0,9,15},new int[]{6,14,19},new int[]{1,11,1});
  Analyzer a3=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field){
      Tokenizer tokenizer=new MockTokenizer(MockTokenizer.WHITESPACE,false);
      StopFilter filter=new StopFilter(tokenizer,StandardAnalyzer.STOP_WORDS_SET);
      return new TokenStreamComponents(tokenizer,new WordDelimiterFilter(filter,flags,protWords));
    }
  }
;
  assertAnalyzesTo(a3,"lucene.solr",new String[]{"lucene","lucenesolr","solr"},new int[]{0,0,7},new int[]{6,11,11},new int[]{1,0,1});
  assertAnalyzesTo(a3,"the lucene.solr",new String[]{"lucene","lucenesolr","solr"},new int[]{4,4,11},new int[]{10,15,15},new int[]{2,0,1});
}
