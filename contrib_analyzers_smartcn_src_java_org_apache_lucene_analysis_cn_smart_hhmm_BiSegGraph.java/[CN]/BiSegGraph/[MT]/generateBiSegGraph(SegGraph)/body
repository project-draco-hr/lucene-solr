{
  double smooth=0.1;
  int wordPairFreq=0;
  int maxStart=segGraph.getMaxStart();
  double oneWordFreq, weight, tinyDouble=1.0 / Utility.MAX_FREQUENCE;
  int next;
  char[] idBuffer;
  segTokenList=segGraph.makeIndex();
  int key=-1;
  List nextTokens=null;
  while (key < maxStart) {
    if (segGraph.isStartExist(key)) {
      List tokenList=segGraph.getStartList(key);
      for (Iterator iter=tokenList.iterator(); iter.hasNext(); ) {
        SegToken t1=(SegToken)iter.next();
        oneWordFreq=t1.weight;
        next=t1.endOffset;
        nextTokens=null;
        while (next <= maxStart) {
          if (segGraph.isStartExist(next)) {
            nextTokens=segGraph.getStartList(next);
            break;
          }
          next++;
        }
        if (nextTokens == null) {
          break;
        }
        for (Iterator iter2=nextTokens.iterator(); iter2.hasNext(); ) {
          SegToken t2=(SegToken)iter2.next();
          idBuffer=new char[t1.charArray.length + t2.charArray.length + 1];
          System.arraycopy(t1.charArray,0,idBuffer,0,t1.charArray.length);
          idBuffer[t1.charArray.length]=BigramDictionary.WORD_SEGMENT_CHAR;
          System.arraycopy(t2.charArray,0,idBuffer,t1.charArray.length + 1,t2.charArray.length);
          wordPairFreq=bigramDict.getFrequency(idBuffer);
          weight=-Math.log(smooth * (1.0 + oneWordFreq) / (Utility.MAX_FREQUENCE + 0.0) + (1.0 - smooth) * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));
          SegTokenPair tokenPair=new SegTokenPair(idBuffer,t1.index,t2.index,weight);
          this.addSegTokenPair(tokenPair);
        }
      }
    }
    key++;
  }
}
