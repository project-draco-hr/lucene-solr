{
  final int flags=GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL| SPLIT_ON_CASE_CHANGE| SPLIT_ON_NUMERICS| STEM_ENGLISH_POSSESSIVE;
  final CharArraySet protWords=new CharArraySet(new HashSet<>(Arrays.asList("NUTCH")),false);
  Analyzer a=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field){
      Tokenizer tokenizer=new MockTokenizer(MockTokenizer.WHITESPACE,false);
      return new TokenStreamComponents(tokenizer,new Lucene47WordDelimiterFilter(tokenizer,flags,protWords));
    }
  }
;
  assertAnalyzesTo(a,"LUCENE / SOLR",new String[]{"LUCENE","SOLR"},new int[]{0,9},new int[]{6,13},null,new int[]{1,1},null,false);
  assertAnalyzesTo(a,"LUCENE / solR",new String[]{"LUCENE","sol","R","solR"},new int[]{0,9,12,9},new int[]{6,12,13,13},null,new int[]{1,1,1,0},null,false);
  assertAnalyzesTo(a,"LUCENE / NUTCH SOLR",new String[]{"LUCENE","NUTCH","SOLR"},new int[]{0,9,15},new int[]{6,14,19},null,new int[]{1,1,1},null,false);
  Analyzer a2=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field){
      Tokenizer tokenizer=new MockTokenizer(MockTokenizer.WHITESPACE,false);
      return new TokenStreamComponents(tokenizer,new Lucene47WordDelimiterFilter(new LargePosIncTokenFilter(tokenizer),flags,protWords));
    }
  }
;
  assertAnalyzesTo(a2,"LUCENE largegap SOLR",new String[]{"LUCENE","largegap","SOLR"},new int[]{0,7,16},new int[]{6,15,20},null,new int[]{1,10,1},null,false);
  assertAnalyzesTo(a2,"LUCENE / SOLR",new String[]{"LUCENE","SOLR"},new int[]{0,9},new int[]{6,13},null,new int[]{1,11},null,false);
  assertAnalyzesTo(a2,"LUCENE / solR",new String[]{"LUCENE","sol","R","solR"},new int[]{0,9,12,9},new int[]{6,12,13,13},null,new int[]{1,11,1,0},null,false);
  assertAnalyzesTo(a2,"LUCENE / NUTCH SOLR",new String[]{"LUCENE","NUTCH","SOLR"},new int[]{0,9,15},new int[]{6,14,19},null,new int[]{1,11,1},null,false);
  Analyzer a3=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field){
      Tokenizer tokenizer=new MockTokenizer(MockTokenizer.WHITESPACE,false);
      StopFilter filter=new StopFilter(tokenizer,StandardAnalyzer.STOP_WORDS_SET);
      return new TokenStreamComponents(tokenizer,new Lucene47WordDelimiterFilter(filter,flags,protWords));
    }
  }
;
  assertAnalyzesTo(a3,"lucene.solr",new String[]{"lucene","solr","lucenesolr"},new int[]{0,7,0},new int[]{6,11,11},null,new int[]{1,1,0},null,false);
  assertAnalyzesTo(a3,"the lucene.solr",new String[]{"lucene","solr","lucenesolr"},new int[]{4,11,4},new int[]{10,15,15},null,new int[]{2,1,0},null,false);
}
