{
  Directory dir=newDirectory();
  RandomIndexWriter w=new RandomIndexWriter(random,dir,new StringSplitAnalyzer());
  char[] chars=new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];
  Arrays.fill(chars,'x');
  Document doc=new Document();
  final String bigTerm=new String(chars);
  final BytesRef bigTermBytesRef=new BytesRef(bigTerm);
  String contents="abc xyz x" + bigTerm + " another term";
  doc.add(new Field("content",contents,Field.Store.NO,Field.Index.ANALYZED));
  w.addDocument(doc);
  doc=new Document();
  doc.add(new Field("content","abc bbb ccc",Field.Store.NO,Field.Index.ANALYZED));
  w.addDocument(doc);
  IndexReader reader=w.getReader();
  w.close();
  assertEquals(2,reader.docFreq(new Term("content","abc")));
  assertEquals(1,reader.docFreq(new Term("content","bbb")));
  assertEquals(1,reader.docFreq(new Term("content","term")));
  assertEquals(1,reader.docFreq(new Term("content","another")));
  DocsAndPositionsEnum tps=MultiFields.getTermPositionsEnum(reader,null,"content",new BytesRef("another"));
  assertEquals(0,tps.nextDoc());
  assertEquals(1,tps.freq());
  assertEquals(3,tps.nextPosition());
  assertEquals("document with wicked long term should is not in the index!",2,reader.numDocs());
  reader.close();
  dir.close();
  dir=newDirectory();
  doc=new Document();
  Field contentField=new Field("content","",Field.Store.NO,Field.Index.NOT_ANALYZED);
  doc.add(contentField);
  w=new RandomIndexWriter(random,dir);
  contentField.setValue("other");
  w.addDocument(doc);
  contentField.setValue("term");
  w.addDocument(doc);
  contentField.setValue(bigTerm);
  w.addDocument(doc);
  contentField.setValue("zzz");
  w.addDocument(doc);
  reader=w.getReader();
  w.close();
  assertEquals(1,reader.docFreq(new Term("content",bigTerm)));
  FieldCache.DocTermsIndex dti=FieldCache.DEFAULT.getTermsIndex(reader,"content",random.nextBoolean());
  assertEquals(5,dti.numOrd());
  assertEquals(4,dti.size());
  assertEquals(bigTermBytesRef,dti.lookup(3,new BytesRef()));
  reader.close();
  dir.close();
}
