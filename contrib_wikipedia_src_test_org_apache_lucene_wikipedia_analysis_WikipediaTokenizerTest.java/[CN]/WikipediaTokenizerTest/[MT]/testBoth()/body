{
  Set untoks=new HashSet();
  untoks.add(WikipediaTokenizer.CATEGORY);
  untoks.add(WikipediaTokenizer.ITALICS);
  String test="[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
  WikipediaTokenizer tf=new WikipediaTokenizer(new StringReader(test),WikipediaTokenizer.BOTH,untoks);
  Token token;
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "a b c d",new String(token.termBuffer(),0,token.termLength()).equals("a b c d") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG,token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
  assertTrue(token.startOffset() + " does not equal: " + 11,token.startOffset() == 11);
  assertTrue(token.endOffset() + " does not equal: " + 18,token.endOffset() == 18);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "a",new String(token.termBuffer(),0,token.termLength()).equals("a") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 0,token.getPositionIncrement() == 0);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.getFlags() + " equals: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG+ " and it shouldn't",token.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
  assertTrue(token.startOffset() + " does not equal: " + 11,token.startOffset() == 11);
  assertTrue(token.endOffset() + " does not equal: " + 12,token.endOffset() == 12);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "b",new String(token.termBuffer(),0,token.termLength()).equals("b") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.startOffset() + " does not equal: " + 13,token.startOffset() == 13);
  assertTrue(token.endOffset() + " does not equal: " + 14,token.endOffset() == 14);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "c",new String(token.termBuffer(),0,token.termLength()).equals("c") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.startOffset() + " does not equal: " + 15,token.startOffset() == 15);
  assertTrue(token.endOffset() + " does not equal: " + 16,token.endOffset() == 16);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "d",new String(token.termBuffer(),0,token.termLength()).equals("d") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.startOffset() + " does not equal: " + 17,token.startOffset() == 17);
  assertTrue(token.endOffset() + " does not equal: " + 18,token.endOffset() == 18);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "e f g",new String(token.termBuffer(),0,token.termLength()).equals("e f g") == true);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG,token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
  assertTrue(token.startOffset() + " does not equal: " + 32,token.startOffset() == 32);
  assertTrue(token.endOffset() + " does not equal: " + 37,token.endOffset() == 37);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "e",new String(token.termBuffer(),0,token.termLength()).equals("e") == true);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 0,token.getPositionIncrement() == 0);
  assertTrue(token.startOffset() + " does not equal: " + 32,token.startOffset() == 32);
  assertTrue(token.endOffset() + " does not equal: " + 33,token.endOffset() == 33);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "f",new String(token.termBuffer(),0,token.termLength()).equals("f") == true);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.startOffset() + " does not equal: " + 34,token.startOffset() == 34);
  assertTrue(token.endOffset() + " does not equal: " + 35,token.endOffset() == 35);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "g",new String(token.termBuffer(),0,token.termLength()).equals("g") == true);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.startOffset() + " does not equal: " + 36,token.startOffset() == 36);
  assertTrue(token.endOffset() + " does not equal: " + 37,token.endOffset() == 37);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "link",new String(token.termBuffer(),0,token.termLength()).equals("link") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK,token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
  assertTrue(token.startOffset() + " does not equal: " + 42,token.startOffset() == 42);
  assertTrue(token.endOffset() + " does not equal: " + 46,token.endOffset() == 46);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "here",new String(token.termBuffer(),0,token.termLength()).equals("here") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK,token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
  assertTrue(token.startOffset() + " does not equal: " + 47,token.startOffset() == 47);
  assertTrue(token.endOffset() + " does not equal: " + 51,token.endOffset() == 51);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "link",new String(token.termBuffer(),0,token.termLength()).equals("link") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.startOffset() + " does not equal: " + 56,token.startOffset() == 56);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK,token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
  assertTrue(token.endOffset() + " does not equal: " + 60,token.endOffset() == 60);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "there",new String(token.termBuffer(),0,token.termLength()).equals("there") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK,token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
  assertTrue(token.startOffset() + " does not equal: " + 61,token.startOffset() == 61);
  assertTrue(token.endOffset() + " does not equal: " + 66,token.endOffset() == 66);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "italics here",new String(token.termBuffer(),0,token.termLength()).equals("italics here") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS,token.type().equals(WikipediaTokenizer.ITALICS) == true);
  assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG,token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
  assertTrue(token.startOffset() + " does not equal: " + 71,token.startOffset() == 71);
  assertTrue(token.endOffset() + " does not equal: " + 83,token.endOffset() == 83);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "italics",new String(token.termBuffer(),0,token.termLength()).equals("italics") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 0,token.getPositionIncrement() == 0);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS,token.type().equals(WikipediaTokenizer.ITALICS) == true);
  assertTrue(token.startOffset() + " does not equal: " + 71,token.startOffset() == 71);
  assertTrue(token.endOffset() + " does not equal: " + 78,token.endOffset() == 78);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "here",new String(token.termBuffer(),0,token.termLength()).equals("here") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS,token.type().equals(WikipediaTokenizer.ITALICS) == true);
  assertTrue(token.startOffset() + " does not equal: " + 79,token.startOffset() == 79);
  assertTrue(token.endOffset() + " does not equal: " + 83,token.endOffset() == 83);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "something",new String(token.termBuffer(),0,token.termLength()).equals("something") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.startOffset() + " does not equal: " + 86,token.startOffset() == 86);
  assertTrue(token.endOffset() + " does not equal: " + 95,token.endOffset() == 95);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "more italics",new String(token.termBuffer(),0,token.termLength()).equals("more italics") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS,token.type().equals(WikipediaTokenizer.ITALICS) == true);
  assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG,token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
  assertTrue(token.startOffset() + " does not equal: " + 98,token.startOffset() == 98);
  assertTrue(token.endOffset() + " does not equal: " + 110,token.endOffset() == 110);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "more",new String(token.termBuffer(),0,token.termLength()).equals("more") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 0,token.getPositionIncrement() == 0);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS,token.type().equals(WikipediaTokenizer.ITALICS) == true);
  assertTrue(token.startOffset() + " does not equal: " + 98,token.startOffset() == 98);
  assertTrue(token.endOffset() + " does not equal: " + 102,token.endOffset() == 102);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "italics",new String(token.termBuffer(),0,token.termLength()).equals("italics") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS,token.type().equals(WikipediaTokenizer.ITALICS) == true);
  assertTrue(token.startOffset() + " does not equal: " + 103,token.startOffset() == 103);
  assertTrue(token.endOffset() + " does not equal: " + 110,token.endOffset() == 110);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "h   i   j",new String(token.termBuffer(),0,token.termLength()).equals("h   i   j") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG,token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
  assertTrue(token.startOffset() + " does not equal: " + 124,token.startOffset() == 124);
  assertTrue(token.endOffset() + " does not equal: " + 133,token.endOffset() == 133);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "h",new String(token.termBuffer(),0,token.termLength()).equals("h") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 0,token.getPositionIncrement() == 0);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.startOffset() + " does not equal: " + 124,token.startOffset() == 124);
  assertTrue(token.endOffset() + " does not equal: " + 125,token.endOffset() == 125);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "i",new String(token.termBuffer(),0,token.termLength()).equals("i") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.startOffset() + " does not equal: " + 128,token.startOffset() == 128);
  assertTrue(token.endOffset() + " does not equal: " + 129,token.endOffset() == 129);
  token=tf.next();
  assertTrue("token is null and it shouldn't be",token != null);
  assertTrue(new String(token.termBuffer(),0,token.termLength()) + " is not equal to " + "j",new String(token.termBuffer(),0,token.termLength()).equals("j") == true);
  assertTrue(token.getPositionIncrement() + " does not equal: " + 1,token.getPositionIncrement() == 1);
  assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY,token.type().equals(WikipediaTokenizer.CATEGORY) == true);
  assertTrue(token.startOffset() + " does not equal: " + 132,token.startOffset() == 132);
  assertTrue(token.endOffset() + " does not equal: " + 133,token.endOffset() == 133);
  token=tf.next();
  assertTrue("token is not null and it should be",token == null);
}
