{
  waitForThingsToLevelOut(15);
  ClusterState clusterState=cloudClient.getZkStateReader().getClusterState();
  DocRouter router=clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();
  Slice shard1=clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION,SHARD1);
  DocRouter.Range shard1Range=shard1.getRange() != null ? shard1.getRange() : router.fullRange();
  final List<DocRouter.Range> ranges=router.partitionRange(2,shard1Range);
  final int[] docCounts=new int[ranges.size()];
  int numReplicas=shard1.getReplicas().size();
  del("*:*");
  for (int id=0; id < 100; id++) {
    indexAndUpdateCount(ranges,docCounts,id);
  }
  commit();
  Thread indexThread=new Thread(){
    @Override public void run(){
      int max=atLeast(401);
      for (int id=101; id < max; id++) {
        try {
          indexAndUpdateCount(ranges,docCounts,id);
          Thread.sleep(atLeast(25));
        }
 catch (        Exception e) {
          log.error("Exception while adding doc",e);
        }
      }
    }
  }
;
  indexThread.start();
  splitShard(SHARD1);
  log.info("Layout after split: \n");
  printLayout();
  indexThread.join();
  commit();
  checkDocCountsAndShardStates(docCounts,numReplicas);
  waitForRecoveriesToFinish(true);
}
