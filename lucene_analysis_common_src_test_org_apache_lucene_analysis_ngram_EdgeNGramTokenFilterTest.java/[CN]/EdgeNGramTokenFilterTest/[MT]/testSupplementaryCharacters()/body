{
  final String s=_TestUtil.randomUnicodeString(random(),10);
  final int codePointCount=s.codePointCount(0,s.length());
  final int minGram=_TestUtil.nextInt(random(),1,3);
  final int maxGram=_TestUtil.nextInt(random(),minGram,10);
  TokenStream tk=new KeywordTokenizer(new StringReader(s));
  tk=new EdgeNGramTokenFilter(TEST_VERSION_CURRENT,tk,minGram,maxGram);
  final CharTermAttribute termAtt=tk.addAttribute(CharTermAttribute.class);
  final OffsetAttribute offsetAtt=tk.addAttribute(OffsetAttribute.class);
  tk.reset();
  for (int i=minGram; i <= Math.min(codePointCount,maxGram); ++i) {
    assertTrue(tk.incrementToken());
    assertEquals(0,offsetAtt.startOffset());
    assertEquals(s.length(),offsetAtt.endOffset());
    final int end=Character.offsetByCodePoints(s,0,i);
    assertEquals(s.substring(0,end),termAtt.toString());
  }
  assertFalse(tk.incrementToken());
}
