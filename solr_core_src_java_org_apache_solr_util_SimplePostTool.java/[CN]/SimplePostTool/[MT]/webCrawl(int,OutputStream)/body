{
  int numPages=0;
  LinkedHashSet<URL> stack=backlog.get(level);
  int rawStackSize=stack.size();
  stack.removeAll(visited);
  int stackSize=stack.size();
  LinkedHashSet<URL> subStack=new LinkedHashSet<>();
  info("Entering crawl at level " + level + " ("+ rawStackSize+ " links total, "+ stackSize+ " new)");
  for (  URL u : stack) {
    try {
      visited.add(u);
      PageFetcherResult result=pageFetcher.readPageFromUrl(u);
      if (result.httpStatus == 200) {
        u=(result.redirectUrl != null) ? result.redirectUrl : u;
        URL postUrl=new URL(appendParam(solrUrl.toString(),"literal.id=" + URLEncoder.encode(u.toString(),"UTF-8") + "&literal.url="+ URLEncoder.encode(u.toString(),"UTF-8")));
        boolean success=postData(new ByteArrayInputStream(result.content.array(),result.content.arrayOffset(),result.content.limit()),null,out,result.contentType,postUrl);
        if (success) {
          info("POSTed web resource " + u + " (depth: "+ level+ ")");
          Thread.sleep(delay * 1000);
          numPages++;
          if (recursive > level && result.contentType.equals("text/html")) {
            Set<URL> children=pageFetcher.getLinksFromWebPage(u,new ByteArrayInputStream(result.content.array(),result.content.arrayOffset(),result.content.limit()),result.contentType,postUrl);
            subStack.addAll(children);
          }
        }
 else {
          warn("An error occurred while posting " + u);
        }
      }
 else {
        warn("The URL " + u + " returned a HTTP result status of "+ result.httpStatus);
      }
    }
 catch (    IOException e) {
      warn("Caught exception when trying to open connection to " + u + ": "+ e.getMessage());
    }
catch (    InterruptedException e) {
      throw new RuntimeException();
    }
  }
  if (!subStack.isEmpty()) {
    backlog.add(subStack);
    numPages+=webCrawl(level + 1,out);
  }
  return numPages;
}
