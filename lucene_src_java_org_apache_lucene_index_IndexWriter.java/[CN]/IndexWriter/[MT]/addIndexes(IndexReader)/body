{
  ensureOpen();
  int numDocs=0;
  try {
    if (infoStream != null)     message("flush at addIndexes(IndexReader...)");
    flush(false,true);
    String mergedName=newSegmentName();
    for (    IndexReader indexReader : readers) {
      numDocs+=indexReader.numDocs();
    }
    final IOContext context=new IOContext(new MergeInfo(numDocs,-1,true,false));
    SegmentMerger merger=new SegmentMerger(directory,config.getTermIndexInterval(),mergedName,null,payloadProcessorProvider,new FieldInfos(globalFieldNumberMap),codec,context);
    for (    IndexReader reader : readers)     merger.add(reader);
    int docCount=merger.merge();
    final FieldInfos fieldInfos=merger.fieldInfos();
    SegmentInfo info=new SegmentInfo(mergedName,docCount,directory,false,merger.getCodec(),fieldInfos);
    setDiagnostics(info,"addIndexes(IndexReader...)");
    boolean useCompoundFile;
synchronized (this) {
      if (stopMerges) {
        deleter.deleteNewFiles(info.files());
        return;
      }
      ensureOpen();
      useCompoundFile=mergePolicy.useCompoundFile(segmentInfos,info);
    }
    if (useCompoundFile) {
      merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName,"",IndexFileNames.COMPOUND_FILE_EXTENSION),info,context);
synchronized (this) {
        deleter.deleteNewFiles(info.files());
      }
      info.setUseCompoundFile(true);
    }
synchronized (this) {
      if (stopMerges) {
        deleter.deleteNewFiles(info.files());
        return;
      }
      ensureOpen();
      segmentInfos.add(info);
      checkpoint();
    }
  }
 catch (  OutOfMemoryError oom) {
    handleOOM(oom,"addIndexes(IndexReader...)");
  }
}
