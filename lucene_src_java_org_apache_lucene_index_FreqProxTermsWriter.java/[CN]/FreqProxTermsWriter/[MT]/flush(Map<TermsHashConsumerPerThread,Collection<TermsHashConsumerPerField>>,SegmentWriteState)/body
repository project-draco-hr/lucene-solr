{
  List<FreqProxTermsWriterPerField> allFields=new ArrayList<FreqProxTermsWriterPerField>();
  flushedDocCount=state.numDocs;
  for (  Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {
    Collection<TermsHashConsumerPerField> fields=entry.getValue();
    for (    final TermsHashConsumerPerField i : fields) {
      final FreqProxTermsWriterPerField perField=(FreqProxTermsWriterPerField)i;
      if (perField.termsHashPerField.bytesHash.size() > 0)       allFields.add(perField);
    }
  }
  final int numAllFields=allFields.size();
  Collections.sort(allFields);
  final FieldsConsumer consumer=state.codec.fieldsConsumer(state);
  int start=0;
  while (start < numAllFields) {
    final FieldInfo fieldInfo=allFields.get(start).fieldInfo;
    final String fieldName=fieldInfo.name;
    int end=start + 1;
    while (end < numAllFields && allFields.get(end).fieldInfo.name.equals(fieldName))     end++;
    FreqProxTermsWriterPerField[] fields=new FreqProxTermsWriterPerField[end - start];
    for (int i=start; i < end; i++) {
      fields[i - start]=allFields.get(i);
      fieldInfo.storePayloads|=fields[i - start].hasPayloads;
    }
    appendPostings(fields,consumer);
    for (int i=0; i < fields.length; i++) {
      TermsHashPerField perField=fields[i].termsHashPerField;
      int numPostings=perField.bytesHash.size();
      perField.reset();
      perField.shrinkHash(numPostings);
      fields[i].reset();
    }
    start=end;
  }
  for (  Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {
    FreqProxTermsWriterPerThread perThread=(FreqProxTermsWriterPerThread)entry.getKey();
    perThread.termsHashPerThread.reset(true);
  }
  consumer.close();
}
