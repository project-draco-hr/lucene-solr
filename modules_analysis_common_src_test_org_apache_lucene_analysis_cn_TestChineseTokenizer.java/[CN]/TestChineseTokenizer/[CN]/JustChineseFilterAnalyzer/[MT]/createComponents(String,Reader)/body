{
  Tokenizer tokenizer=new WhitespaceTokenizer(Version.LUCENE_CURRENT,reader);
  return new TokenStreamComponents(tokenizer,new ChineseFilter(tokenizer));
}
