{
  RAMDirectory dir=new RAMDirectory();
  Analyzer analyzer=new ClassicAnalyzer();
  IndexWriter writer=new IndexWriter(dir,new IndexWriterConfig(analyzer));
  char[] chars=new char[IndexWriter.MAX_TERM_LENGTH];
  Arrays.fill(chars,'x');
  Document doc=new Document();
  final String bigTerm=new String(chars);
  String contents="abc xyz x" + bigTerm + " another term";
  doc.add(new TextField("content",contents,Field.Store.NO));
  writer.addDocument(doc);
  doc=new Document();
  doc.add(new TextField("content","abc bbb ccc",Field.Store.NO));
  writer.addDocument(doc);
  writer.close();
  IndexReader reader=DirectoryReader.open(dir);
  assertEquals(2,reader.docFreq(new Term("content","abc")));
  assertEquals(1,reader.docFreq(new Term("content","bbb")));
  assertEquals(1,reader.docFreq(new Term("content","term")));
  assertEquals(1,reader.docFreq(new Term("content","another")));
  PostingsEnum tps=MultiFields.getTermPositionsEnum(reader,MultiFields.getLiveDocs(reader),"content",new BytesRef("another"));
  assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
  assertEquals(1,tps.freq());
  assertEquals(3,tps.nextPosition());
  assertEquals("document with wicked long term should is not in the index!",2,reader.numDocs());
  reader.close();
  doc=new Document();
  doc.add(new TextField("content",bigTerm,Field.Store.NO));
  ClassicAnalyzer sa=new ClassicAnalyzer();
  sa.setMaxTokenLength(100000);
  writer=new IndexWriter(dir,new IndexWriterConfig(sa));
  writer.addDocument(doc);
  writer.close();
  reader=DirectoryReader.open(dir);
  assertEquals(1,reader.docFreq(new Term("content",bigTerm)));
  reader.close();
  dir.close();
  analyzer.close();
  sa.close();
}
