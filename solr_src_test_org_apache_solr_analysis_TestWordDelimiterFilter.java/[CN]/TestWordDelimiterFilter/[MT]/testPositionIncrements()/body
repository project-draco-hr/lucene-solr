{
  final CharArraySet protWords=new CharArraySet(DEFAULT_VERSION,new HashSet<String>(Arrays.asList("NUTCH")),false);
  Analyzer a=new Analyzer(){
    public TokenStream tokenStream(    String field,    Reader reader){
      return new WordDelimiterFilter(new WhitespaceTokenizer(DEFAULT_VERSION,reader),1,1,0,0,1,1,0,1,1,protWords);
    }
  }
;
  assertAnalyzesTo(a,"LUCENE / SOLR",new String[]{"LUCENE","SOLR"},new int[]{0,9},new int[]{6,13},new int[]{1,1});
  assertAnalyzesTo(a,"LUCENE / solR",new String[]{"LUCENE","sol","R","solR"},new int[]{0,9,12,9},new int[]{6,12,13,13},new int[]{1,1,1,0});
  assertAnalyzesTo(a,"LUCENE / NUTCH SOLR",new String[]{"LUCENE","NUTCH","SOLR"},new int[]{0,9,15},new int[]{6,14,19},new int[]{1,1,1});
  Analyzer a2=new Analyzer(){
    public TokenStream tokenStream(    String field,    Reader reader){
      return new WordDelimiterFilter(new LargePosIncTokenFilter(new WhitespaceTokenizer(DEFAULT_VERSION,reader)),1,1,0,0,1,1,0,1,1,protWords);
    }
  }
;
  assertAnalyzesTo(a2,"LUCENE largegap SOLR",new String[]{"LUCENE","largegap","SOLR"},new int[]{0,7,16},new int[]{6,15,20},new int[]{1,10,1});
  assertAnalyzesTo(a2,"LUCENE / SOLR",new String[]{"LUCENE","SOLR"},new int[]{0,9},new int[]{6,13},new int[]{1,11});
  assertAnalyzesTo(a2,"LUCENE / solR",new String[]{"LUCENE","sol","R","solR"},new int[]{0,9,12,9},new int[]{6,12,13,13},new int[]{1,11,1,0});
  assertAnalyzesTo(a2,"LUCENE / NUTCH SOLR",new String[]{"LUCENE","NUTCH","SOLR"},new int[]{0,9,15},new int[]{6,14,19},new int[]{1,11,1});
}
