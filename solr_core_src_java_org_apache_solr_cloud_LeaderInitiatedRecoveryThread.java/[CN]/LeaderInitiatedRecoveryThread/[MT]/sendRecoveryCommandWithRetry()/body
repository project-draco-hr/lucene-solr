{
  int tries=0;
  long waitBetweenTriesMs=5000L;
  boolean continueTrying=true;
  String recoveryUrl=nodeProps.getBaseUrl();
  String replicaNodeName=nodeProps.getNodeName();
  String coreNeedingRecovery=nodeProps.getCoreName();
  String replicaCoreNodeName=((Replica)nodeProps.getNodeProps()).getName();
  String replicaUrl=nodeProps.getCoreUrl();
  log.info(getName() + " started running to send REQUESTRECOVERY command to " + replicaUrl+ "; will try for a max of "+ (maxTries * (waitBetweenTriesMs / 1000))+ " secs");
  RequestRecovery recoverRequestCmd=new RequestRecovery();
  recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);
  recoverRequestCmd.setCoreName(coreNeedingRecovery);
  while (continueTrying && ++tries <= maxTries) {
    if (tries > 1) {
      log.warn("Asking core={} coreNodeName={} on " + recoveryUrl + " to recover; unsuccessful after "+ tries+ " of "+ maxTries+ " attempts so far ...",coreNeedingRecovery,replicaCoreNodeName);
    }
 else {
      log.info("Asking core={} coreNodeName={} on " + recoveryUrl + " to recover",coreNeedingRecovery,replicaCoreNodeName);
    }
    try (HttpSolrClient client=new HttpSolrClient(recoveryUrl)){
      client.setSoTimeout(60000);
      client.setConnectionTimeout(15000);
      try {
        client.request(recoverRequestCmd);
        log.info("Successfully sent " + CoreAdminAction.REQUESTRECOVERY + " command to core={} coreNodeName={} on "+ recoveryUrl,coreNeedingRecovery,replicaCoreNodeName);
        continueTrying=false;
      }
 catch (      Throwable t) {
        Throwable rootCause=SolrException.getRootCause(t);
        boolean wasCommError=(rootCause instanceof ConnectException || rootCause instanceof ConnectTimeoutException || rootCause instanceof NoHttpResponseException|| rootCause instanceof SocketException);
        SolrException.log(log,recoveryUrl + ": Could not tell a replica to recover",t);
        if (!wasCommError) {
          continueTrying=false;
        }
      }
    }
     if (continueTrying) {
      try {
        Thread.sleep(waitBetweenTriesMs);
      }
 catch (      InterruptedException ignoreMe) {
        Thread.currentThread().interrupt();
      }
      if (coreContainer.isShutDown()) {
        log.warn("Stop trying to send recovery command to downed replica core={} coreNodeName={} on " + replicaNodeName + " because my core container is closed.",coreNeedingRecovery,replicaCoreNodeName);
        continueTrying=false;
        break;
      }
      ZkStateReader zkStateReader=zkController.getZkStateReader();
      try {
        zkStateReader.updateClusterState(true);
      }
 catch (      Exception exc) {
        log.warn("Error when updating cluster state: " + exc);
      }
      if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {
        log.warn("Node " + replicaNodeName + " hosting core "+ coreNeedingRecovery+ " is no longer live. No need to keep trying to tell it to recover!");
        continueTrying=false;
        break;
      }
      if (leaderCoreNodeName != null && collection != null) {
        String leaderCoreNodeNameFromZk=null;
        try {
          leaderCoreNodeNameFromZk=zkController.getZkStateReader().getLeaderRetry(collection,shardId,1000).getName();
        }
 catch (        Exception exc) {
          log.error("Failed to determine if " + leaderCoreNodeName + " is still the leader for "+ collection+ " "+ shardId+ " before starting leader-initiated recovery thread for "+ replicaUrl+ " due to: "+ exc);
        }
        if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {
          log.warn("Stop trying to send recovery command to downed replica core=" + coreNeedingRecovery + ",coreNodeName="+ replicaCoreNodeName+ " on "+ replicaNodeName+ " because "+ leaderCoreNodeName+ " is no longer the leader! New leader is "+ leaderCoreNodeNameFromZk);
          continueTrying=false;
          break;
        }
      }
      if (collection != null && shardId != null) {
        try {
          final Replica.State lirState=zkController.getLeaderInitiatedRecoveryState(collection,shardId,replicaCoreNodeName);
          if (lirState == null) {
            log.warn("Stop trying to send recovery command to downed replica core=" + coreNeedingRecovery + ",coreNodeName="+ replicaCoreNodeName+ " on "+ replicaNodeName+ " because the znode no longer exists.");
            continueTrying=false;
            break;
          }
          if (lirState == Replica.State.RECOVERING) {
            continueTrying=false;
            log.info("Replica " + coreNeedingRecovery + " on node "+ replicaNodeName+ " ack'd the leader initiated recovery state, "+ "no need to keep trying to send recovery command");
          }
 else {
            String leaderCoreNodeName=zkStateReader.getLeaderRetry(collection,shardId,5000).getName();
            List<ZkCoreNodeProps> replicaProps=zkStateReader.getReplicaProps(collection,shardId,leaderCoreNodeName);
            if (replicaProps != null && replicaProps.size() > 0) {
              for (              ZkCoreNodeProps prop : replicaProps) {
                final Replica replica=(Replica)prop.getNodeProps();
                if (replicaCoreNodeName.equals(replica.getName())) {
                  if (replica.getState() == Replica.State.ACTIVE) {
                    if (lirState == Replica.State.DOWN) {
                      log.warn("Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;" + " forcing it back to down state to re-run the leader-initiated recovery process; props: " + replicaProps.get(0),coreNeedingRecovery,replicaCoreNodeName);
                      zkController.ensureReplicaInLeaderInitiatedRecovery(collection,shardId,nodeProps,leaderCoreNodeName,true,true);
                    }
                  }
                  break;
                }
              }
            }
          }
        }
 catch (        Exception ignoreMe) {
          log.warn("Failed to determine state of core={} coreNodeName={} due to: " + ignoreMe,coreNeedingRecovery,replicaCoreNodeName);
        }
      }
    }
  }
  zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);
  if (continueTrying) {
    log.error("Timed out after waiting for " + (tries * (waitBetweenTriesMs / 1000)) + " secs to send the recovery request to: "+ replicaUrl+ "; not much more we can do here?");
  }
}
