{
  Map joinParams=new HashMap();
  Set<String> flSet=new HashSet();
  flSet.add(gather);
  flSet.add(traverseTo);
  if (metrics != null) {
    for (    Metric metric : metrics) {
      for (      String column : metric.getColumns()) {
        flSet.add(column);
      }
    }
  }
  if (queryParams.containsKey("fl")) {
    String flString=queryParams.get("fl");
    String[] flArray=flString.split(",");
    for (    String f : flArray) {
      flSet.add(f.trim());
    }
  }
  Iterator<String> it=flSet.iterator();
  StringBuilder buf=new StringBuilder();
  while (it.hasNext()) {
    buf.append(it.next());
    if (it.hasNext()) {
      buf.append(",");
    }
  }
  joinParams.putAll(queryParams);
  joinParams.put("fl",buf.toString());
  joinParams.put("qt","/export");
  joinParams.put("sort",gather + " asc," + traverseTo+ " asc");
  StringBuffer nodeQuery=new StringBuffer();
  boolean comma=false;
  for (  String node : nodes) {
    if (comma) {
      nodeQuery.append(",");
    }
    nodeQuery.append(node);
    comma=true;
  }
  if (maxDocFreq > -1) {
    String docFreqParam=" maxDocFreq=" + maxDocFreq;
    joinParams.put("q","{!graphTerms f=" + traverseTo + docFreqParam+ "}"+ nodeQuery.toString());
  }
 else {
    joinParams.put("q","{!terms f=" + traverseTo + "}"+ nodeQuery.toString());
  }
  TupleStream stream=null;
  try {
    stream=new UniqueStream(new CloudSolrStream(zkHost,collection,joinParams),new MultipleFieldEqualitor(new FieldEqualitor(gather),new FieldEqualitor(traverseTo)));
    stream.setStreamContext(streamContext);
    stream.open();
    BATCH:     while (true) {
      Tuple tuple=stream.read();
      if (tuple.EOF) {
        break BATCH;
      }
      edges.add(tuple);
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
 finally {
    try {
      stream.close();
    }
 catch (    Exception ce) {
      throw new RuntimeException(ce);
    }
  }
  return edges;
}
