{
  if ("PreFlex".equals(CodecProvider.getDefault().getDefaultFieldCodec())) {
    throw new RuntimeException("thist test cannot run with PreFlex codec");
  }
  long TERM_COUNT=((long)Integer.MAX_VALUE) + 100000000;
  int TERMS_PER_DOC=1000000;
  Directory dir=FSDirectory.open(_TestUtil.getTempDir("2BTerms"));
  IndexWriter w=new IndexWriter(dir,new IndexWriterConfig(TEST_VERSION_CURRENT,new MockAnalyzer()).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setRAMBufferSizeMB(256.0).setMergeScheduler(new ConcurrentMergeScheduler()));
  ((LogMergePolicy)w.getConfig().getMergePolicy()).setUseCompoundFile(false);
  ((LogMergePolicy)w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);
  ((LogMergePolicy)w.getConfig().getMergePolicy()).setMergeFactor(10);
  Document doc=new Document();
  Field field=new Field("field",new MyTokenStream(TERMS_PER_DOC));
  field.setOmitTermFreqAndPositions(true);
  field.setOmitNorms(true);
  doc.add(field);
  final int numDocs=(int)(TERM_COUNT / TERMS_PER_DOC);
  for (int i=0; i < numDocs; i++) {
    final long t0=System.currentTimeMillis();
    w.addDocument(doc);
    System.out.println(i + " of " + numDocs+ " "+ (System.currentTimeMillis() - t0)+ " msec");
  }
  System.out.println("now optimize...");
  w.optimize();
  w.close();
  System.out.println("now CheckIndex...");
  CheckIndex.Status status=_TestUtil.checkIndex(dir);
  final long tc=status.segmentInfos.get(0).termIndexStatus.termCount;
  assertTrue("count " + tc + " is not > "+ Integer.MAX_VALUE,tc > Integer.MAX_VALUE);
  dir.close();
}
