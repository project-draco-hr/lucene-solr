{
  TokenStream stream=new MockTokenizer(new StringReader("short toolong evenmuchlongertext a ab toolong foo"),MockTokenizer.WHITESPACE,false);
  OffsetLimitTokenFilter filter=new OffsetLimitTokenFilter(stream,10);
  assertTokenStreamContents(filter,new String[]{"short","toolong"});
  stream=new MockTokenizer(new StringReader("short toolong evenmuchlongertext a ab toolong foo"),MockTokenizer.WHITESPACE,false);
  filter=new OffsetLimitTokenFilter(stream,12);
  assertTokenStreamContents(filter,new String[]{"short","toolong"});
  stream=new MockTokenizer(new StringReader("short toolong evenmuchlongertext a ab toolong foo"),MockTokenizer.WHITESPACE,false);
  filter=new OffsetLimitTokenFilter(stream,30);
  assertTokenStreamContents(filter,new String[]{"short","toolong","evenmuchlongertext"});
  checkOneTermReuse(new Analyzer(){
    @Override public TokenStream tokenStream(    String fieldName,    Reader reader){
      return new OffsetLimitTokenFilter(new MockTokenizer(reader,MockTokenizer.WHITESPACE,false),10);
    }
  }
,"llenges","llenges");
}
