{
  Tokenizer tokenizer=new KeywordTokenizer(new StringReader(""));
  TokenFilter filter=new GermanStemFilter(new LowerCaseFilter(Version.LUCENE_CURRENT,tokenizer));
  File dataDir=new File(System.getProperty("dataDir","./bin"));
  File testFile=new File(dataDir,"org/apache/lucene/analysis/de/data.txt");
  FileInputStream fis=new FileInputStream(testFile);
  InputStreamReader isr=new InputStreamReader(fis,"iso-8859-1");
  BufferedReader breader=new BufferedReader(isr);
  while (true) {
    String line=breader.readLine();
    if (line == null)     break;
    line=line.trim();
    if (line.startsWith("#") || line.equals(""))     continue;
    String[] parts=line.split(";");
    tokenizer.reset(new StringReader(parts[0]));
    filter.reset();
    assertTokenStreamContents(filter,new String[]{parts[1]});
  }
  breader.close();
  isr.close();
  fis.close();
}
