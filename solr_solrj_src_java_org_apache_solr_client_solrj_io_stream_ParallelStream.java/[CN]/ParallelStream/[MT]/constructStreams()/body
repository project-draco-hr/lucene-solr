{
  try {
    Object pushStream=null;
    if (objectSerialize) {
      ByteArrayOutputStream bout=new ByteArrayOutputStream();
      ObjectOutputStream out=new ObjectOutputStream(bout);
      out.writeObject(tupleStream);
      byte[] bytes=bout.toByteArray();
      String encoded=Base64.byteArrayToBase64(bytes,0,bytes.length);
      pushStream=URLEncoder.encode(encoded,"UTF-8");
    }
 else {
      pushStream=((Expressible)tupleStream).toExpression(streamFactory);
    }
    ZkStateReader zkStateReader=cloudSolrClient.getZkStateReader();
    ClusterState clusterState=zkStateReader.getClusterState();
    Collection<Slice> slices=clusterState.getActiveSlices(this.collection);
    long time=System.currentTimeMillis();
    List<Replica> shuffler=new ArrayList();
    for (    Slice slice : slices) {
      Collection<Replica> replicas=slice.getReplicas();
      for (      Replica replica : replicas) {
        shuffler.add(replica);
      }
    }
    if (workers > shuffler.size()) {
      throw new IOException("Number of workers exceeds nodes in the worker collection");
    }
    Collections.shuffle(shuffler,new Random(time));
    for (int w=0; w < workers; w++) {
      HashMap params=new HashMap();
      params.put("distrib","false");
      params.put("numWorkers",workers);
      params.put("workerID",w);
      params.put("stream",pushStream);
      params.put("qt","/stream");
      params.put("objectSerialize",objectSerialize);
      Replica rep=shuffler.get(w);
      ZkCoreNodeProps zkProps=new ZkCoreNodeProps(rep);
      String url=zkProps.getCoreUrl();
      SolrStream solrStream=new SolrStream(url,params);
      solrStreams.add(solrStream);
    }
    assert(solrStreams.size() == workers);
  }
 catch (  Exception e) {
    throw new IOException(e);
  }
}
