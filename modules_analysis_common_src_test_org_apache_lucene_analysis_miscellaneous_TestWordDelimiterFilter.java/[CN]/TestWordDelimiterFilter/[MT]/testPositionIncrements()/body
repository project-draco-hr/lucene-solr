{
  final int flags=GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL| SPLIT_ON_CASE_CHANGE| SPLIT_ON_NUMERICS| STEM_ENGLISH_POSSESSIVE;
  final CharArraySet protWords=new CharArraySet(TEST_VERSION_CURRENT,new HashSet<String>(Arrays.asList("NUTCH")),false);
  Analyzer a=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field,    Reader reader){
      Tokenizer tokenizer=new MockTokenizer(reader,MockTokenizer.WHITESPACE,false);
      return new TokenStreamComponents(tokenizer,new WordDelimiterFilter(tokenizer,flags,protWords));
    }
  }
;
  assertAnalyzesTo(a,"LUCENE / SOLR",new String[]{"LUCENE","SOLR"},new int[]{0,9},new int[]{6,13},new int[]{1,1});
  assertAnalyzesTo(a,"LUCENE / solR",new String[]{"LUCENE","sol","R","solR"},new int[]{0,9,12,9},new int[]{6,12,13,13},new int[]{1,1,1,0});
  assertAnalyzesTo(a,"LUCENE / NUTCH SOLR",new String[]{"LUCENE","NUTCH","SOLR"},new int[]{0,9,15},new int[]{6,14,19},new int[]{1,1,1});
  Analyzer a2=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field,    Reader reader){
      Tokenizer tokenizer=new MockTokenizer(reader,MockTokenizer.WHITESPACE,false);
      return new TokenStreamComponents(tokenizer,new WordDelimiterFilter(new LargePosIncTokenFilter(tokenizer),flags,protWords));
    }
  }
;
  assertAnalyzesTo(a2,"LUCENE largegap SOLR",new String[]{"LUCENE","largegap","SOLR"},new int[]{0,7,16},new int[]{6,15,20},new int[]{1,10,1});
  assertAnalyzesTo(a2,"LUCENE / SOLR",new String[]{"LUCENE","SOLR"},new int[]{0,9},new int[]{6,13},new int[]{1,11});
  assertAnalyzesTo(a2,"LUCENE / solR",new String[]{"LUCENE","sol","R","solR"},new int[]{0,9,12,9},new int[]{6,12,13,13},new int[]{1,11,1,0});
  assertAnalyzesTo(a2,"LUCENE / NUTCH SOLR",new String[]{"LUCENE","NUTCH","SOLR"},new int[]{0,9,15},new int[]{6,14,19},new int[]{1,11,1});
  Analyzer a3=new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String field,    Reader reader){
      Tokenizer tokenizer=new MockTokenizer(reader,MockTokenizer.WHITESPACE,false);
      StopFilter filter=new StopFilter(TEST_VERSION_CURRENT,tokenizer,StandardAnalyzer.STOP_WORDS_SET);
      filter.setEnablePositionIncrements(true);
      return new TokenStreamComponents(tokenizer,new WordDelimiterFilter(filter,flags,protWords));
    }
  }
;
  assertAnalyzesTo(a3,"lucene.solr",new String[]{"lucene","solr","lucenesolr"},new int[]{0,7,0},new int[]{6,11,11},new int[]{1,1,0});
  assertAnalyzesTo(a3,"the lucene.solr",new String[]{"lucene","solr","lucenesolr"},new int[]{4,11,4},new int[]{10,15,15},new int[]{2,1,0});
}
