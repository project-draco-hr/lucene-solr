{
  if (terms.size() == 0)   return null;
  final IndexReader reader=context.reader;
  PostingsAndFreq[] postingsFreqs=new PostingsAndFreq[terms.size()];
  final Bits delDocs=reader.getDeletedDocs();
  for (int i=0; i < terms.size(); i++) {
    final Term t=terms.get(i);
    DocsAndPositionsEnum postingsEnum=reader.termPositionsEnum(delDocs,t.field(),t.bytes());
    if (postingsEnum == null) {
      if (reader.termDocsEnum(delDocs,t.field(),t.bytes()) != null) {
        throw new IllegalStateException("field \"" + t.field() + "\" was indexed with Field.omitTermFreqAndPositions=true; cannot run PhraseQuery (term="+ t.text()+ ")");
      }
 else {
        return null;
      }
    }
    postingsFreqs[i]=new PostingsAndFreq(postingsEnum,reader.docFreq(t.field(),t.bytes()),positions.get(i).intValue());
  }
  if (slop == 0) {
    ArrayUtil.quickSort(postingsFreqs);
  }
  if (slop == 0) {
    ExactPhraseScorer s=new ExactPhraseScorer(this,postingsFreqs,similarity,reader.norms(field));
    if (s.noDocs) {
      return null;
    }
 else {
      return s;
    }
  }
 else {
    return new SloppyPhraseScorer(this,postingsFreqs,similarity,slop,reader.norms(field));
  }
}
