{
  int nrThreads=10;
  ArrayList startURLs=new ArrayList();
  String restrictTo=".*";
  boolean gui=false;
  boolean showInfo=false;
  String hostResolverFile="";
  System.out.println("LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - B.20020914");
  for (int i=0; i < args.length; i++) {
    if (args[i].equals("-start")) {
      i++;
      String arg=args[i];
      if (arg.startsWith("@")) {
        String fileName=arg.substring(1);
        System.out.println("reading URL file " + fileName);
        try {
          BufferedReader r=new BufferedReader(new FileReader(fileName));
          String line;
          int count=0;
          while ((line=r.readLine()) != null) {
            try {
              startURLs.add(new URL(line));
              count++;
            }
 catch (            MalformedURLException e) {
              System.out.println("Malformed URL '" + line + "' in line "+ (count + 1)+ " of file "+ fileName);
            }
          }
          r.close();
          System.out.println("added " + count + " URLs from "+ fileName);
        }
 catch (        IOException e) {
          System.out.println("Couldn't read '" + fileName + "': "+ e);
        }
      }
 else {
        System.out.println("got URL " + arg);
        try {
          startURLs.add(new URL(arg));
          System.out.println("Start-URL added: " + arg);
        }
 catch (        MalformedURLException e) {
          System.out.println("Malformed URL '" + arg + "'");
        }
      }
    }
 else     if (args[i].equals("-restrictto")) {
      i++;
      restrictTo=args[i];
      System.out.println("Restricting URLs to " + restrictTo);
    }
 else     if (args[i].equals("-threads")) {
      i++;
      nrThreads=Integer.parseInt(args[i]);
      System.out.println("Threads set to " + nrThreads);
    }
 else     if (args[i].equals("-hostresolver")) {
      i++;
      hostResolverFile=args[i];
      System.out.println("reading host resolver props from  '" + hostResolverFile + "'");
    }
 else     if (args[i].equals("-gui")) {
      gui=true;
    }
 else     if (args[i].equals("-?")) {
      showInfo=true;
    }
 else {
      System.out.println("Unknown option: " + args[i] + "; use -? to get syntax");
      System.exit(0);
    }
  }
  FetcherMain f=new FetcherMain(nrThreads,hostResolverFile);
  if (showInfo || "".equals(hostResolverFile) || (startURLs.isEmpty() && gui == false)) {
    System.out.println("The LARM crawler\n" + "\n" + "The LARM crawler is a fast parallel crawler, currently designed for\n"+ "large intranets (up to a couple hundred hosts with some hundred thousand\n"+ "documents). It is currently restricted by a relatively high memory overhead\n"+ "per crawled host, and by a HashMap of already crawled URLs which is also held\n"+ "in memory.\n"+ "\n"+ "Usage:   FetcherMain <-start <URL>|@<filename>>+ -restrictto <RegEx>\n"+ "                    [-threads <nr=10>] [-hostresolver <filename>]\n"+ "\n"+ "Commands:\n"+ "         -start specify one or more URLs to start with. You can as well specify a file"+ "                that contains URLs, one each line\n"+ "         -restrictto a Perl 5 regular expression each URL must match. It is run against the\n"+ "                     _complete_ URL, including the http:// part\n"+ "         -threads  the number of crawling threads. defaults to 10\n"+ "         -hostresolver specify a file that contains rules for changing the host part of \n"+ "                       a URL during the normalization process (experimental).\n"+ "Caution: The <RegEx> is applied to the _normalized_ form of a URL.\n"+ "         See URLNormalizer for details\n"+ "Example:\n"+ "    -start @urls1.txt -start @urls2.txt -start http://localhost/ "+ "    -restrictto http://[^/]*\\.localhost/.* -threads 25\n"+ "\n"+ "The host resolver file may contain the following commands: \n"+ "  startsWith(part1) = part2\n"+ "      if host starts with part1, this part will be replaced by part2\n"+ "   endsWith(part1) = part2\n"+ "       if host ends with part1, this part will be replaced by part2. This is done after\n"+ "       startsWith was processed\n"+ "   synonym(host1) = host2\n"+ "       the keywords startsWith, endsWith and synonym are case sensitive\n"+ "       host1 will be replaced with host2. this is done _after_ startsWith and endsWith was \n"+ "       processed. Due to a bug in BeanUtils, dots are not allowed in the keys (in parentheses)\n"+ "       and have to be escaped with commas. To simplify, commas are also replaced in property \n"+ "       values. So just use commas instead of dots. The resulting host names are only used for \n"+ "       comparisons and do not have to be existing URLs (although the syntax has to be valid).\n"+ "       However, the names will often be passed to java.net.URL which will try to make a DNS name\n"+ "       resolution, which will time out if the server can't be found. \n"+ "   Example:"+ "     synonym(www1,host,com) = host,com\n"+ "     startsWith(www,) = ,\n"+ "     endsWith(host1,com) = host,com\n"+ "The crawler will show a status message every 5 seconds, which is printed by ThreadMonitor.java\n"+ "It will stop after the ThreadMonitor found the message queue and the crawling threads to be idle a \n"+ "couple of times.\n"+ "The crawled data will be saved within a logs/ directory. A cachingqueue/ directory is used for\n"+ "temporary queues.\n"+ "Note that this implementation is experimental, and that the command line options cover only a part \n"+ "of the parameters. Much of the configuration can only be done by modifying FetcherMain.java\n");
    System.exit(0);
  }
  try {
    f.setRexString(restrictTo);
    if (gui) {
    }
 else {
      f.startMonitor();
      for (Iterator it=startURLs.iterator(); it.hasNext(); ) {
        f.putURL((URL)it.next(),false);
      }
    }
  }
 catch (  MalformedPatternException e) {
    System.out.println("Wrong RegEx syntax. Must be a valid PERL RE");
  }
}
