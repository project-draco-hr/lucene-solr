{
  MockTokenizer stream=new MockTokenizer(new StringReader("short toolong evenmuchlongertext a ab toolong foo"),MockTokenizer.WHITESPACE,false);
  stream.setEnableChecks(false);
  OffsetLimitTokenFilter filter=new OffsetLimitTokenFilter(stream,10);
  assertTokenStreamContents(filter,new String[]{"short","toolong"});
  stream=new MockTokenizer(new StringReader("short toolong evenmuchlongertext a ab toolong foo"),MockTokenizer.WHITESPACE,false);
  stream.setEnableChecks(false);
  filter=new OffsetLimitTokenFilter(stream,12);
  assertTokenStreamContents(filter,new String[]{"short","toolong"});
  stream=new MockTokenizer(new StringReader("short toolong evenmuchlongertext a ab toolong foo"),MockTokenizer.WHITESPACE,false);
  stream.setEnableChecks(false);
  filter=new OffsetLimitTokenFilter(stream,30);
  assertTokenStreamContents(filter,new String[]{"short","toolong","evenmuchlongertext"});
  checkOneTermReuse(new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String fieldName,    Reader reader){
      MockTokenizer tokenizer=new MockTokenizer(reader,MockTokenizer.WHITESPACE,false);
      tokenizer.setEnableChecks(false);
      return new TokenStreamComponents(tokenizer,new OffsetLimitTokenFilter(tokenizer,10));
    }
  }
,"llenges","llenges");
}
