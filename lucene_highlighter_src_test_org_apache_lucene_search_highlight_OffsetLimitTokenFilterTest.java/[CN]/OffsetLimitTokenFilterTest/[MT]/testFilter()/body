{
  MockTokenizer stream=new MockTokenizer(MockTokenizer.WHITESPACE,false);
  stream.setReader(new StringReader("short toolong evenmuchlongertext a ab toolong foo"));
  stream.setEnableChecks(false);
  OffsetLimitTokenFilter filter=new OffsetLimitTokenFilter(stream,10);
  assertTokenStreamContents(filter,new String[]{"short","toolong"});
  stream=new MockTokenizer(MockTokenizer.WHITESPACE,false);
  stream.setReader(new StringReader("short toolong evenmuchlongertext a ab toolong foo"));
  stream.setEnableChecks(false);
  filter=new OffsetLimitTokenFilter(stream,12);
  assertTokenStreamContents(filter,new String[]{"short","toolong"});
  stream=new MockTokenizer(MockTokenizer.WHITESPACE,false);
  stream.setReader(new StringReader("short toolong evenmuchlongertext a ab toolong foo"));
  stream.setEnableChecks(false);
  filter=new OffsetLimitTokenFilter(stream,30);
  assertTokenStreamContents(filter,new String[]{"short","toolong","evenmuchlongertext"});
  checkOneTerm(new Analyzer(){
    @Override public TokenStreamComponents createComponents(    String fieldName){
      MockTokenizer tokenizer=new MockTokenizer(MockTokenizer.WHITESPACE,false);
      tokenizer.setEnableChecks(false);
      return new TokenStreamComponents(tokenizer,new OffsetLimitTokenFilter(tokenizer,10));
    }
  }
,"llenges","llenges");
}
