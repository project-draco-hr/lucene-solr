{
  super.setUp();
  Arrays.sort(testTerms);
  int tokenUpto=0;
  for (int i=0; i < testTerms.length; i++) {
    positions[i]=new int[TERM_FREQ];
    offsets[i]=new TermVectorOffsetInfo[TERM_FREQ];
    for (int j=0; j < TERM_FREQ; j++) {
      positions[i][j]=(int)(j * 10 + Math.random() * 10);
      offsets[i][j]=new TermVectorOffsetInfo(j * 10,j * 10 + testTerms[i].length());
      TestToken token=tokens[tokenUpto++]=new TestToken();
      token.text=testTerms[i];
      token.pos=positions[i][j];
      token.startOffset=offsets[i][j].getStartOffset();
      token.endOffset=offsets[i][j].getEndOffset();
    }
  }
  Arrays.sort(tokens);
  IndexWriter writer=new IndexWriter(dir,new MyAnalyzer(),true,IndexWriter.MaxFieldLength.LIMITED);
  writer.setUseCompoundFile(false);
  Document doc=new Document();
  for (int i=0; i < testFields.length; i++) {
    final Field.TermVector tv;
    if (testFieldsStorePos[i] && testFieldsStoreOff[i])     tv=Field.TermVector.WITH_POSITIONS_OFFSETS;
 else     if (testFieldsStorePos[i] && !testFieldsStoreOff[i])     tv=Field.TermVector.WITH_POSITIONS;
 else     if (!testFieldsStorePos[i] && testFieldsStoreOff[i])     tv=Field.TermVector.WITH_OFFSETS;
 else     tv=Field.TermVector.YES;
    doc.add(new Field(testFields[i],"",Field.Store.NO,Field.Index.ANALYZED,tv));
  }
  for (int j=0; j < 5; j++)   writer.addDocument(doc);
  writer.flush();
  seg=writer.newestSegment().name;
  writer.close();
  fieldInfos=new FieldInfos(dir,seg + "." + IndexFileNames.FIELD_INFOS_EXTENSION);
}
