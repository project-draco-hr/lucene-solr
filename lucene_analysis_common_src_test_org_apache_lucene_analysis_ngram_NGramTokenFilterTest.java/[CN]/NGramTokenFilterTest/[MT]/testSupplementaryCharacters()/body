{
  final String s=TestUtil.randomUnicodeString(random(),10);
  final int codePointCount=s.codePointCount(0,s.length());
  final int minGram=TestUtil.nextInt(random(),1,3);
  final int maxGram=TestUtil.nextInt(random(),minGram,10);
  TokenStream tk=new KeywordTokenizer();
  ((Tokenizer)tk).setReader(new StringReader(s));
  tk=new NGramTokenFilter(tk,minGram,maxGram);
  final CharTermAttribute termAtt=tk.addAttribute(CharTermAttribute.class);
  final OffsetAttribute offsetAtt=tk.addAttribute(OffsetAttribute.class);
  tk.reset();
  for (int start=0; start < codePointCount; ++start) {
    for (int end=start + minGram; end <= Math.min(codePointCount,start + maxGram); ++end) {
      assertTrue(tk.incrementToken());
      assertEquals(0,offsetAtt.startOffset());
      assertEquals(s.length(),offsetAtt.endOffset());
      final int startIndex=Character.offsetByCodePoints(s,0,start);
      final int endIndex=Character.offsetByCodePoints(s,0,end);
      assertEquals(s.substring(startIndex,endIndex),termAtt.toString());
    }
  }
  assertFalse(tk.incrementToken());
}
