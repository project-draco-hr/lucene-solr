{
  Directory dir=new RAMDirectory();
  RandomIndexWriter writer=new RandomIndexWriter(newRandom(),dir);
  Document doc=new Document();
  TokenStream stream=new TokenStream(){
    private int index=0;
    private CharTermAttribute termAtt=addAttribute(CharTermAttribute.class);
    private OffsetAttribute offsetAtt=addAttribute(OffsetAttribute.class);
    @Override public boolean incrementToken() throws IOException {
      if (index == tokens.length) {
        return false;
      }
 else {
        clearAttributes();
        termAtt.append(tokens[index++]);
        offsetAtt.setOffset(0,0);
        return true;
      }
    }
  }
;
  stream=new CachingTokenFilter(stream);
  doc.add(new Field("preanalyzed",stream,TermVector.NO));
  checkTokens(stream);
  stream.reset();
  checkTokens(stream);
  writer.addDocument(doc);
  IndexReader reader=writer.getReader();
  DocsAndPositionsEnum termPositions=MultiFields.getTermPositionsEnum(reader,MultiFields.getDeletedDocs(reader),"preanalyzed",new BytesRef("term1"));
  assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);
  assertEquals(1,termPositions.freq());
  assertEquals(0,termPositions.nextPosition());
  termPositions=MultiFields.getTermPositionsEnum(reader,MultiFields.getDeletedDocs(reader),"preanalyzed",new BytesRef("term2"));
  assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);
  assertEquals(2,termPositions.freq());
  assertEquals(1,termPositions.nextPosition());
  assertEquals(3,termPositions.nextPosition());
  termPositions=MultiFields.getTermPositionsEnum(reader,MultiFields.getDeletedDocs(reader),"preanalyzed",new BytesRef("term3"));
  assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);
  assertEquals(1,termPositions.freq());
  assertEquals(2,termPositions.nextPosition());
  reader.close();
  writer.close();
  stream.reset();
  checkTokens(stream);
}
