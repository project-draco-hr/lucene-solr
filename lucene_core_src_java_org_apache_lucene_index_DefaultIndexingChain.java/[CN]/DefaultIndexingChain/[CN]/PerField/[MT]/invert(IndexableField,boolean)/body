{
  if (first) {
    invertState.reset();
  }
  IndexableFieldType fieldType=field.fieldType();
  IndexOptions indexOptions=fieldType.indexOptions();
  fieldInfo.setIndexOptions(indexOptions);
  if (fieldType.omitNorms()) {
    fieldInfo.setOmitsNorms();
  }
  final boolean analyzed=fieldType.tokenized() && docState.analyzer != null;
  final boolean checkOffsets=indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
  boolean succeededInProcessingField=false;
  try (TokenStream stream=tokenStream=field.tokenStream(docState.analyzer,tokenStream)){
    stream.reset();
    invertState.setAttributeSource(stream);
    termsHashPerField.start(field,first);
    while (stream.incrementToken()) {
      int posIncr=invertState.posIncrAttribute.getPositionIncrement();
      invertState.position+=posIncr;
      if (invertState.position < invertState.lastPosition) {
        if (posIncr == 0) {
          throw new IllegalArgumentException("first position increment must be > 0 (got 0) for field '" + field.name() + "'");
        }
        throw new IllegalArgumentException("position increments (and gaps) must be >= 0 (got " + posIncr + ") for field '"+ field.name()+ "'");
      }
      invertState.lastPosition=invertState.position;
      if (posIncr == 0) {
        invertState.numOverlap++;
      }
      if (checkOffsets) {
        int startOffset=invertState.offset + invertState.offsetAttribute.startOffset();
        int endOffset=invertState.offset + invertState.offsetAttribute.endOffset();
        if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {
          throw new IllegalArgumentException("startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards " + "startOffset=" + startOffset + ",endOffset="+ endOffset+ ",lastStartOffset="+ invertState.lastStartOffset+ " for field '"+ field.name()+ "'");
        }
        invertState.lastStartOffset=startOffset;
      }
      invertState.length++;
      if (invertState.length < 0) {
        throw new IllegalArgumentException("too many tokens in field '" + field.name() + "'");
      }
      try {
        termsHashPerField.add();
      }
 catch (      MaxBytesLengthExceededException e) {
        byte[] prefix=new byte[30];
        BytesRef bigTerm=invertState.termAttribute.getBytesRef();
        System.arraycopy(bigTerm.bytes,bigTerm.offset,prefix,0,30);
        String msg="Document contains at least one immense term in field=\"" + fieldInfo.name + "\" (whose UTF8 encoding is longer than the max length "+ DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8+ "), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '"+ Arrays.toString(prefix)+ "...', original message: "+ e.getMessage();
        if (docState.infoStream.isEnabled("IW")) {
          docState.infoStream.message("IW","ERROR: " + msg);
        }
        throw new IllegalArgumentException(msg,e);
      }
catch (      Throwable th) {
        throw new AbortingException(th);
      }
    }
    stream.end();
    invertState.position+=invertState.posIncrAttribute.getPositionIncrement();
    invertState.offset+=invertState.offsetAttribute.endOffset();
    succeededInProcessingField=true;
  }
  finally {
    if (!succeededInProcessingField && docState.infoStream.isEnabled("DW")) {
      docState.infoStream.message("DW","An exception was thrown while processing field " + fieldInfo.name);
    }
  }
  if (analyzed) {
    invertState.position+=docState.analyzer.getPositionIncrementGap(fieldInfo.name);
    invertState.offset+=docState.analyzer.getOffsetGap(fieldInfo.name);
  }
  invertState.boost*=field.boost();
}
